import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import Counter
import string
import re
import seaborn as sns
from tqdm import tqdm
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings("ignore")

def train(size, init_scale):
    
    seed = 0
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    alpha = init_scale

    is_cuda = torch.cuda.is_available()

    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.
    if is_cuda:
        device = torch.device("cuda")
        print("GPU is available")
    else:
        device = torch.device("cpu")
        print("GPU not available, CPU used")

    base_csv = './IMDB Dataset.csv'
    df = pd.read_csv(base_csv)
    #df.head()

    #size = 1000
    X,y = df[:size]['review'].values,df[:size]['sentiment'].values
    x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)
    #print(f'shape of train data is {x_train.shape}')
    #print(f'shape of test data is {x_test.shape}')

    def preprocess_string(s):
        # Remove all non-word characters (everything except numbers and letters)
        s = re.sub(r"[^\w\s]", '', s)
        # Replace all runs of whitespaces with no space
        s = re.sub(r"\s+", '', s)
        # replace digits with no space
        s = re.sub(r"\d", '', s)

        return s

    def tockenize(x_train,y_train,x_val,y_val):
        word_list = []

        stop_words = {'a',
                     'about',
                     'above',
                     'after',
                     'again',
                     'against',
                     'ain',
                     'all',
                     'am',
                     'an',
                     'and',
                     'any',
                     'are',
                     'aren',
                     "aren't",
                     'as',
                     'at',
                     'be',
                     'because',
                     'been',
                     'before',
                     'being',
                     'below',
                     'between',
                     'both',
                     'but',
                     'by',
                     'can',
                     'couldn',
                     "couldn't",
                     'd',
                     'did',
                     'didn',
                     "didn't",
                     'do',
                     'does',
                     'doesn',
                     "doesn't",
                     'doing',
                     'don',
                     "don't",
                     'down',
                     'during',
                     'each',
                     'few',
                     'for',
                     'from',
                     'further',
                     'had',
                     'hadn',
                     "hadn't",
                     'has',
                     'hasn',
                     "hasn't",
                     'have',
                     'haven',
                     "haven't",
                     'having',
                     'he',
                     'her',
                     'here',
                     'hers',
                     'herself',
                     'him',
                     'himself',
                     'his',
                     'how',
                     'i',
                     'if',
                     'in',
                     'into',
                     'is',
                     'isn',
                     "isn't",
                     'it',
                     "it's",
                     'its',
                     'itself',
                     'just',
                     'll',
                     'm',
                     'ma',
                     'me',
                     'mightn',
                     "mightn't",
                     'more',
                     'most',
                     'mustn',
                     "mustn't",
                     'my',
                     'myself',
                     'needn',
                     "needn't",
                     'no',
                     'nor',
                     'not',
                     'now',
                     'o',
                     'of',
                     'off',
                     'on',
                     'once',
                     'only',
                     'or',
                     'other',
                     'our',
                     'ours',
                     'ourselves',
                     'out',
                     'over',
                     'own',
                     're',
                     's',
                     'same',
                     'shan',
                     "shan't",
                     'she',
                     "she's",
                     'should',
                     "should've",
                     'shouldn',
                     "shouldn't",
                     'so',
                     'some',
                     'such',
                     't',
                     'than',
                     'that',
                     "that'll",
                     'the',
                     'their',
                     'theirs',
                     'them',
                     'themselves',
                     'then',
                     'there',
                     'these',
                     'they',
                     'this',
                     'those',
                     'through',
                     'to',
                     'too',
                     'under',
                     'until',
                     'up',
                     've',
                     'very',
                     'was',
                     'wasn',
                     "wasn't",
                     'we',
                     'were',
                     'weren',
                     "weren't",
                     'what',
                     'when',
                     'where',
                     'which',
                     'while',
                     'who',
                     'whom',
                     'why',
                     'will',
                     'with',
                     'won',
                     "won't",
                     'wouldn',
                     "wouldn't",
                     'y',
                     'you',
                     "you'd",
                     "you'll",
                     "you're",
                     "you've",
                     'your',
                     'yours',
                     'yourself',
                     'yourselves'}

        for sent in x_train:
            for word in sent.lower().split():
                word = preprocess_string(word)
                if word not in stop_words and word != '':
                    word_list.append(word)

        corpus = Counter(word_list)
        # sorting on the basis of most common words
        corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]
        # creating a dict
        onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}

        # tockenize
        final_list_train,final_list_test = [],[]
        for sent in x_train:
                final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() 
                                         if preprocess_string(word) in onehot_dict.keys()])
        for sent in x_val:
                final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() 
                                        if preprocess_string(word) in onehot_dict.keys()])

        encoded_train = [1 if label =='positive' else 0 for label in y_train]  
        encoded_test = [1 if label =='positive' else 0 for label in y_val] 
        return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict

    x_train,y_train,x_test,y_test,vocab = tockenize(x_train,y_train,x_test,y_test)


    def padding_(sentences, seq_len):
        features = np.zeros((len(sentences), seq_len),dtype=int)
        for ii, review in enumerate(sentences):
            if len(review) != 0:
                features[ii, -len(review):] = np.array(review)[:seq_len]
        return features


    #we have very less number of reviews with length > 500.
    #So we will consideronly those below it.
    x_train_pad = padding_(x_train,500)
    x_test_pad = padding_(x_test,500)

    # create Tensor datasets
    train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))
    valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))

    # dataloaders
    batch_size = 50

    # make sure to SHUFFLE your data
    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)
    valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)

    # obtain one batch of training data
    dataiter = iter(train_loader)
    sample_x, sample_y = dataiter.next()
    
    def L2(model):
        L2_ = 0.
        for p in model.parameters():
            L2_ += torch.sum(p**2)
        return L2_

    def rescale(model, alpha):
        for p in model.parameters():
            p.data = alpha * p.data


    class SentimentRNN(nn.Module):
        def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.0):
            super(SentimentRNN,self).__init__()

            self.output_dim = output_dim
            self.hidden_dim = hidden_dim

            self.no_layers = no_layers
            self.vocab_size = vocab_size

            # embedding and LSTM layers
            self.embedding = nn.Embedding(vocab_size, embedding_dim)

            #lstm
            self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,
                               num_layers=no_layers, batch_first=True)


            # dropout layer
            self.dropout = nn.Dropout(drop_prob)

            # linear and sigmoid layer
            self.fc = nn.Linear(self.hidden_dim, output_dim)
            self.sig = nn.Sigmoid()

        def forward(self,x,hidden):
            batch_size = x.size(0)
            # embeddings and lstm_out
            embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True
            #print(embeds.shape)  #[50, 500, 1000]
            lstm_out, hidden = self.lstm(embeds, hidden)

            lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) 

            # dropout and fully connected layer
            out = self.dropout(lstm_out)
            out = self.fc(out)

            # sigmoid function
            sig_out = self.sig(out)

            # reshape to be batch_size first
            sig_out = sig_out.view(batch_size, -1)

            sig_out = sig_out[:, -1] # get last batch of labels

            # return last sigmoid output and hidden state
            return sig_out, hidden



        def init_hidden(self, batch_size):
            ''' Initializes hidden state '''
            # Create two new tensors with sizes n_layers x batch_size x hidden_dim,
            # initialized to zero, for hidden state and cell state of LSTM
            h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)
            c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)
            hidden = (h0,c0)
            return hidden


    no_layers = 2
    vocab_size = len(vocab) + 1 #extra 1 for padding
    embedding_dim = 64
    output_dim = 1
    hidden_dim = 256


    model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.0)

    #moving to gpu
    model.to(device)
    
    rescale(model, alpha)
    L2_ = L2(model)

    #print(model)

    # loss and optimization functions
    lr=0.001

    criterion = nn.BCELoss()

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # function to predict accuracy
    def acc(pred,label):
        pred = torch.round(pred.squeeze())
        return torch.sum(pred == label.squeeze()).item()



    clip = 5
    epochs = 5 #int(50*1000/size)
    train_loss_min = np.Inf
    valid_loss_min = np.Inf
    train_acc_max = 0.
    valid_acc_max = 0.
    # train for some number of epochs
    epoch_tr_loss,epoch_vl_loss = [],[]
    epoch_tr_acc,epoch_vl_acc = [],[]

    for epoch in range(epochs):
        train_losses = []
        train_acc = 0.0
        model.train()
        # initialize hidden state 
        h = model.init_hidden(batch_size)
        for inputs, labels in train_loader:
            if inputs.shape[0] < batch_size:
                h = model.init_hidden(inputs.shape[0])

            inputs, labels = inputs.to(device), labels.to(device)   
            # Creating new variables for the hidden state, otherwise
            # we'd backprop through the entire training history
            h = tuple([each.data for each in h])

            model.zero_grad()
            output,h = model(inputs,h)

            # calculate the loss and perform backprop
            loss = criterion(output.squeeze(), labels.float())
            loss.backward()
            train_losses.append(loss.item())
            # calculating accuracy
            accuracy = acc(output,labels)
            train_acc += accuracy
            #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
            nn.utils.clip_grad_norm_(model.parameters(), clip)
            optimizer.step()
            
            L2_new = L2(model)
            rescale(model, torch.sqrt(L2_/L2_new))



        val_h = model.init_hidden(batch_size)
        val_losses = []
        val_acc = 0.0
        model.eval()
        for inputs, labels in valid_loader:
            if inputs.shape[0] < batch_size:
                val_h = model.init_hidden(inputs.shape[0])
            val_h = tuple([each.data for each in val_h])

            inputs, labels = inputs.to(device), labels.to(device)

            output, val_h = model(inputs, val_h)
            val_loss = criterion(output.squeeze(), labels.float())

            val_losses.append(val_loss.item())

            accuracy = acc(output,labels)
            val_acc += accuracy

        epoch_train_loss = np.mean(train_losses)
        epoch_val_loss = np.mean(val_losses)
        epoch_train_acc = train_acc/len(train_loader.dataset)
        epoch_val_acc = val_acc/len(valid_loader.dataset)
        epoch_tr_loss.append(epoch_train_loss)
        epoch_vl_loss.append(epoch_val_loss)
        epoch_tr_acc.append(epoch_train_acc)
        epoch_vl_acc.append(epoch_val_acc)
        print(f'Epoch {epoch+1}') 
        print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')
        print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')
        if epoch_val_loss <= valid_loss_min:
            valid_loss_min = epoch_val_loss
        if epoch_train_loss <= train_loss_min:
            train_loss_min = epoch_train_loss
        if epoch_val_acc >= valid_acc_max:
            valid_acc_max = epoch_val_acc
        if epoch_train_acc >= train_acc_max:
            train_acc_max = epoch_train_acc
        #print(25*'==')
        np.savetxt("./results/trainloss_size_%d_scale_%.4f"%(size, alpha), np.array([train_loss_min]))
        np.savetxt("./results/trainacc_size_%d_scale_%.4f"%(size, alpha), np.array([train_acc_max]))
        np.savetxt("./results/testloss_size_%d_scale_%.4f"%(size, alpha), np.array([valid_loss_min]))
        np.savetxt("./results/testacc_size_%d_scale_%.4f"%(size, alpha), np.array([valid_acc_max]))


if __name__ == "__main__":
    #sizes = [int(size) for size in 10**np.linspace(2, 4, num=11)]
    sizes = [int(size) for size in [10**4.2,10**4.4,10**4.6,50000]]
    init_scales = 10**np.linspace(-1, 1, num=11)
    for size in sizes:
        for init_scale in init_scales:
            print("size={},init={}".format(size, init_scale))
            train(size, init_scale)
