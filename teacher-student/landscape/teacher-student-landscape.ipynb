{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e8f91c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_32309/121145993.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs_train = torch.tensor(torch.normal(0,1,size=(train_size, d_in)), dtype=torch.float, requires_grad=True)\n",
      "/var/folders/6j/b6y80djd4nb5hl73rv3sv8y80000gn/T/ipykernel_32309/121145993.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_train = torch.tensor(teacher(inputs_train), dtype=torch.float, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "d_in = 5\n",
    "d_out = 5\n",
    "train_size = 100\n",
    "test_size = 100\n",
    "w = 100\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, w=w):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(d_in, w)\n",
    "        self.l2 = nn.Linear(w, w)\n",
    "        self.l3 = nn.Linear(w,d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = torch.nn.Tanh()\n",
    "        self.x1 = f(self.l1(x))\n",
    "        self.x2 = f(self.l2(self.x1))\n",
    "        self.x3 = self.l3(self.x2)\n",
    "        return self.x3\n",
    "    \n",
    "teacher = Net()\n",
    "inputs_train = torch.tensor(torch.normal(0,1,size=(train_size, d_in)), dtype=torch.float, requires_grad=True)\n",
    "labels_train = torch.tensor(teacher(inputs_train), dtype=torch.float, requires_grad=True)\n",
    "\n",
    "inputs_test = torch.normal(0,1,size=(test_size, d_in))\n",
    "labels_test = teacher(inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b695f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(model):\n",
    "    params = list(model.parameters())\n",
    "    l2 = 0\n",
    "    for i in range(6):\n",
    "        if i == 0:\n",
    "            params_flatten = params[i].reshape(-1,)\n",
    "        params_flatten = torch.cat([params_flatten, params[i].reshape(-1,)])\n",
    "    l2 = torch.sum(params_flatten**2)\n",
    "    return params_flatten, l2\n",
    "\n",
    "def init(model, alpha):\n",
    "    state_dict = model.state_dict()\n",
    "    modules = [\"l1.weight\", \"l1.bias\", \"l2.weight\", \"l2.bias\", \"l3.weight\", \"l3.bias\"]\n",
    "    for module in modules:\n",
    "        state_dict[module] = state_dict[module] * alpha\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "def init2(model, alpha):\n",
    "    model.l1.weight.data = model.l1.weight * alpha\n",
    "    model.l1.bias.data = model.l1.bias * alpha\n",
    "    model.l2.weight.data = model.l2.weight * alpha\n",
    "    model.l2.bias.data = model.l2.bias * alpha\n",
    "    model.l3.weight.data = model.l3.weight * alpha\n",
    "    model.l3.bias.data = model.l3.bias * alpha\n",
    "\n",
    "    \n",
    "def grad(model):\n",
    "    grads = list(student.parameters())\n",
    "    for i in range(6):\n",
    "        if i == 0:\n",
    "            grad = grads[0].reshape(-1,)\n",
    "        else:\n",
    "            grad = torch.cat([grad, grads[i].reshape(-1,)])\n",
    "    return grad\n",
    "    \n",
    "    \n",
    "def func(a, b, c, d, e, f):\n",
    "    p = [a.view(w,d_in), b, c.view(w,w), d, e.view(d_out, w), f]\n",
    "    \n",
    "    f = torch.nn.Tanh()\n",
    "    x1 = f(F.linear(inputs_train, p[0], p[1]))\n",
    "    x2 = f(F.linear(x1, p[2], p[3]))\n",
    "    out = F.linear(x2, p[4], p[5])\n",
    "    \n",
    "    loss = torch.mean((out-labels_train), dim=1)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1713d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------alpha=0.10000000149011612---------\n",
      "epoch: 0  | Train loss: 0.018496 |  Test loss: 0.018844 | l2: 1.073967\n",
      "epoch: 200  | Train loss: 0.013599 |  Test loss: 0.016045 | l2: 1.073967\n",
      "epoch: 400  | Train loss: 0.013694 |  Test loss: 0.016092 | l2: 1.073967\n",
      "epoch: 600  | Train loss: 0.013755 |  Test loss: 0.016153 | l2: 1.073967\n",
      "epoch: 800  | Train loss: 0.013798 |  Test loss: 0.016118 | l2: 1.073967\n",
      "epoch: 1000  | Train loss: 0.013849 |  Test loss: 0.016147 | l2: 1.073967\n",
      "epoch: 1200  | Train loss: 0.013894 |  Test loss: 0.016227 | l2: 1.073967\n",
      "epoch: 1400  | Train loss: 0.013922 |  Test loss: 0.016265 | l2: 1.073967\n",
      "epoch: 1600  | Train loss: 0.013912 |  Test loss: 0.016215 | l2: 1.073967\n",
      "epoch: 1800  | Train loss: 0.013911 |  Test loss: 0.016158 | l2: 1.073967\n",
      "epoch: 2000  | Train loss: 0.013945 |  Test loss: 0.016213 | l2: 1.073967\n",
      "epoch: 2200  | Train loss: 0.013988 |  Test loss: 0.016325 | l2: 1.073967\n",
      "epoch: 2400  | Train loss: 0.014011 |  Test loss: 0.016361 | l2: 1.073967\n",
      "epoch: 2600  | Train loss: 0.013988 |  Test loss: 0.016284 | l2: 1.073967\n",
      "epoch: 2800  | Train loss: 0.013959 |  Test loss: 0.016143 | l2: 1.073967\n",
      "epoch: 3000  | Train loss: 0.013960 |  Test loss: 0.016052 | l2: 1.073967\n",
      "epoch: 3200  | Train loss: 0.013982 |  Test loss: 0.016170 | l2: 1.073967\n",
      "epoch: 3400  | Train loss: 0.014050 |  Test loss: 0.016398 | l2: 1.073967\n",
      "epoch: 3600  | Train loss: 0.014074 |  Test loss: 0.016433 | l2: 1.073967\n",
      "epoch: 3800  | Train loss: 0.014048 |  Test loss: 0.016386 | l2: 1.073967\n",
      "epoch: 4000  | Train loss: 0.014023 |  Test loss: 0.016231 | l2: 1.073967\n",
      "epoch: 4200  | Train loss: 0.013990 |  Test loss: 0.015903 | l2: 1.073967\n",
      "epoch: 4400  | Train loss: 0.014020 |  Test loss: 0.015215 | l2: 1.073967\n",
      "epoch: 4600  | Train loss: 0.014185 |  Test loss: 0.014603 | l2: 1.073967\n",
      "epoch: 4800  | Train loss: 0.014152 |  Test loss: 0.015688 | l2: 1.073967\n",
      "epoch: 5000  | Train loss: 0.014298 |  Test loss: 0.016531 | l2: 1.073967\n",
      "epoch: 5200  | Train loss: 0.014254 |  Test loss: 0.016522 | l2: 1.073967\n",
      "epoch: 5400  | Train loss: 0.014127 |  Test loss: 0.016477 | l2: 1.073967\n",
      "epoch: 5600  | Train loss: 0.014070 |  Test loss: 0.016317 | l2: 1.073967\n",
      "epoch: 5800  | Train loss: 0.014044 |  Test loss: 0.016225 | l2: 1.073967\n",
      "epoch: 6000  | Train loss: 0.014033 |  Test loss: 0.016240 | l2: 1.073967\n",
      "epoch: 6200  | Train loss: 0.014040 |  Test loss: 0.016313 | l2: 1.073967\n",
      "epoch: 6400  | Train loss: 0.014051 |  Test loss: 0.016363 | l2: 1.073967\n",
      "epoch: 6600  | Train loss: 0.014050 |  Test loss: 0.016363 | l2: 1.073967\n",
      "epoch: 6800  | Train loss: 0.014030 |  Test loss: 0.016311 | l2: 1.073967\n",
      "epoch: 7000  | Train loss: 0.014032 |  Test loss: 0.016280 | l2: 1.073967\n",
      "epoch: 7200  | Train loss: 0.014034 |  Test loss: 0.016282 | l2: 1.073967\n",
      "epoch: 7400  | Train loss: 0.014035 |  Test loss: 0.016300 | l2: 1.073967\n",
      "epoch: 7600  | Train loss: 0.014038 |  Test loss: 0.016308 | l2: 1.073967\n",
      "epoch: 7800  | Train loss: 0.014038 |  Test loss: 0.016309 | l2: 1.073967\n",
      "epoch: 8000  | Train loss: 0.014048 |  Test loss: 0.016324 | l2: 1.073967\n",
      "epoch: 8200  | Train loss: 0.014057 |  Test loss: 0.016339 | l2: 1.073967\n",
      "epoch: 8400  | Train loss: 0.014061 |  Test loss: 0.016345 | l2: 1.073967\n",
      "epoch: 8600  | Train loss: 0.014055 |  Test loss: 0.016322 | l2: 1.073967\n",
      "epoch: 8800  | Train loss: 0.014043 |  Test loss: 0.016276 | l2: 1.073967\n",
      "epoch: 9000  | Train loss: 0.014055 |  Test loss: 0.016279 | l2: 1.073967\n",
      "epoch: 9200  | Train loss: 0.014072 |  Test loss: 0.016329 | l2: 1.073967\n",
      "epoch: 9400  | Train loss: 0.014078 |  Test loss: 0.016372 | l2: 1.073967\n",
      "epoch: 9600  | Train loss: 0.014106 |  Test loss: 0.016389 | l2: 1.073967\n",
      "epoch: 9800  | Train loss: 0.014226 |  Test loss: 0.016421 | l2: 1.073967\n",
      "---------alpha=0.15848931670188904---------\n",
      "epoch: 0  | Train loss: 0.018560 |  Test loss: 0.018979 | l2: 2.697684\n",
      "epoch: 200  | Train loss: 0.006478 |  Test loss: 0.010484 | l2: 2.697684\n",
      "epoch: 400  | Train loss: 0.006523 |  Test loss: 0.010924 | l2: 2.697684\n",
      "epoch: 600  | Train loss: 0.006510 |  Test loss: 0.011006 | l2: 2.697684\n",
      "epoch: 800  | Train loss: 0.006543 |  Test loss: 0.011036 | l2: 2.697684\n",
      "epoch: 1000  | Train loss: 0.006656 |  Test loss: 0.011131 | l2: 2.697683\n",
      "epoch: 1200  | Train loss: 0.006854 |  Test loss: 0.011280 | l2: 2.697684\n",
      "epoch: 1400  | Train loss: 0.007024 |  Test loss: 0.011406 | l2: 2.697684\n",
      "epoch: 1600  | Train loss: 0.007126 |  Test loss: 0.011478 | l2: 2.697684\n",
      "epoch: 1800  | Train loss: 0.007171 |  Test loss: 0.011511 | l2: 2.697684\n",
      "epoch: 2000  | Train loss: 0.007192 |  Test loss: 0.011525 | l2: 2.697683\n",
      "epoch: 2200  | Train loss: 0.007205 |  Test loss: 0.011531 | l2: 2.697684\n",
      "epoch: 2400  | Train loss: 0.007253 |  Test loss: 0.011567 | l2: 2.697683\n",
      "epoch: 2600  | Train loss: 0.007336 |  Test loss: 0.011639 | l2: 2.697684\n",
      "epoch: 2800  | Train loss: 0.007454 |  Test loss: 0.011733 | l2: 2.697683\n",
      "epoch: 3000  | Train loss: 0.007543 |  Test loss: 0.011786 | l2: 2.697684\n",
      "epoch: 3200  | Train loss: 0.007559 |  Test loss: 0.011775 | l2: 2.697684\n",
      "epoch: 3400  | Train loss: 0.007524 |  Test loss: 0.011736 | l2: 2.697683\n",
      "epoch: 3600  | Train loss: 0.007553 |  Test loss: 0.011740 | l2: 2.697683\n",
      "epoch: 3800  | Train loss: 0.007659 |  Test loss: 0.011672 | l2: 2.697683\n",
      "epoch: 4000  | Train loss: 0.007690 |  Test loss: 0.011084 | l2: 2.697684\n",
      "epoch: 4200  | Train loss: 0.007750 |  Test loss: 0.010091 | l2: 2.697684\n",
      "epoch: 4400  | Train loss: 0.007845 |  Test loss: 0.010540 | l2: 2.697684\n",
      "epoch: 4600  | Train loss: 0.007767 |  Test loss: 0.011288 | l2: 2.697684\n",
      "epoch: 4800  | Train loss: 0.007563 |  Test loss: 0.011702 | l2: 2.697684\n",
      "epoch: 5000  | Train loss: 0.007331 |  Test loss: 0.011616 | l2: 2.697683\n",
      "epoch: 5200  | Train loss: 0.007275 |  Test loss: 0.011557 | l2: 2.697683\n",
      "epoch: 5400  | Train loss: 0.007236 |  Test loss: 0.011526 | l2: 2.697683\n",
      "epoch: 5600  | Train loss: 0.007229 |  Test loss: 0.011528 | l2: 2.697684\n",
      "epoch: 5800  | Train loss: 0.007243 |  Test loss: 0.011542 | l2: 2.697684\n",
      "epoch: 6000  | Train loss: 0.007317 |  Test loss: 0.011597 | l2: 2.697684\n",
      "epoch: 6200  | Train loss: 0.007395 |  Test loss: 0.011652 | l2: 2.697684\n",
      "epoch: 6400  | Train loss: 0.007423 |  Test loss: 0.011670 | l2: 2.697684\n",
      "epoch: 6600  | Train loss: 0.007422 |  Test loss: 0.011671 | l2: 2.697684\n",
      "epoch: 6800  | Train loss: 0.007438 |  Test loss: 0.011682 | l2: 2.697684\n",
      "epoch: 7000  | Train loss: 0.007453 |  Test loss: 0.011698 | l2: 2.697684\n",
      "epoch: 7200  | Train loss: 0.007503 |  Test loss: 0.011741 | l2: 2.697684\n",
      "epoch: 7400  | Train loss: 0.007608 |  Test loss: 0.011828 | l2: 2.697684\n",
      "epoch: 7600  | Train loss: 0.007794 |  Test loss: 0.011975 | l2: 2.697683\n",
      "epoch: 7800  | Train loss: 0.007823 |  Test loss: 0.011962 | l2: 2.697684\n",
      "epoch: 8000  | Train loss: 0.007815 |  Test loss: 0.011923 | l2: 2.697684\n",
      "epoch: 8200  | Train loss: 0.007829 |  Test loss: 0.011949 | l2: 2.697684\n",
      "epoch: 8400  | Train loss: 0.007754 |  Test loss: 0.011930 | l2: 2.697684\n",
      "epoch: 8600  | Train loss: 0.007655 |  Test loss: 0.011870 | l2: 2.697684\n",
      "epoch: 8800  | Train loss: 0.007523 |  Test loss: 0.011774 | l2: 2.697683\n",
      "epoch: 9000  | Train loss: 0.007445 |  Test loss: 0.011704 | l2: 2.697684\n",
      "epoch: 9200  | Train loss: 0.007444 |  Test loss: 0.011701 | l2: 2.697684\n",
      "epoch: 9400  | Train loss: 0.007444 |  Test loss: 0.011701 | l2: 2.697684\n",
      "epoch: 9600  | Train loss: 0.007438 |  Test loss: 0.011695 | l2: 2.697684\n",
      "epoch: 9800  | Train loss: 0.007434 |  Test loss: 0.011689 | l2: 2.697683\n",
      "---------alpha=0.25118863582611084---------\n",
      "epoch: 0  | Train loss: 0.018855 |  Test loss: 0.019313 | l2: 6.776275\n",
      "epoch: 200  | Train loss: 0.000679 |  Test loss: 0.000909 | l2: 6.776275\n",
      "epoch: 400  | Train loss: 0.000654 |  Test loss: 0.000878 | l2: 6.776275\n",
      "epoch: 600  | Train loss: 0.000615 |  Test loss: 0.000848 | l2: 6.776275\n",
      "epoch: 800  | Train loss: 0.000589 |  Test loss: 0.000829 | l2: 6.776276\n",
      "epoch: 1000  | Train loss: 0.000591 |  Test loss: 0.000837 | l2: 6.776274\n",
      "epoch: 1200  | Train loss: 0.000603 |  Test loss: 0.000847 | l2: 6.776275\n",
      "epoch: 1400  | Train loss: 0.000606 |  Test loss: 0.000853 | l2: 6.776275\n",
      "epoch: 1600  | Train loss: 0.000607 |  Test loss: 0.000855 | l2: 6.776275\n",
      "epoch: 1800  | Train loss: 0.000602 |  Test loss: 0.000850 | l2: 6.776275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2000  | Train loss: 0.000600 |  Test loss: 0.000847 | l2: 6.776275\n",
      "epoch: 2200  | Train loss: 0.000608 |  Test loss: 0.000854 | l2: 6.776274\n",
      "epoch: 2400  | Train loss: 0.000606 |  Test loss: 0.000855 | l2: 6.776275\n",
      "epoch: 2600  | Train loss: 0.000630 |  Test loss: 0.000878 | l2: 6.776275\n",
      "epoch: 2800  | Train loss: 0.000630 |  Test loss: 0.000883 | l2: 6.776274\n",
      "epoch: 3000  | Train loss: 0.000632 |  Test loss: 0.000886 | l2: 6.776275\n",
      "epoch: 3200  | Train loss: 0.000649 |  Test loss: 0.000898 | l2: 6.776274\n",
      "epoch: 3400  | Train loss: 0.000642 |  Test loss: 0.000895 | l2: 6.776275\n",
      "epoch: 3600  | Train loss: 0.000641 |  Test loss: 0.000897 | l2: 6.776275\n",
      "epoch: 3800  | Train loss: 0.000670 |  Test loss: 0.000930 | l2: 6.776275\n",
      "epoch: 4000  | Train loss: 0.000692 |  Test loss: 0.000963 | l2: 6.776275\n",
      "epoch: 4200  | Train loss: 0.000713 |  Test loss: 0.000990 | l2: 6.776274\n",
      "epoch: 4400  | Train loss: 0.000725 |  Test loss: 0.001017 | l2: 6.776275\n",
      "epoch: 4600  | Train loss: 0.000756 |  Test loss: 0.001062 | l2: 6.776275\n",
      "epoch: 4800  | Train loss: 0.000824 |  Test loss: 0.001177 | l2: 6.776275\n",
      "epoch: 5000  | Train loss: 0.000889 |  Test loss: 0.001292 | l2: 6.776274\n",
      "epoch: 5200  | Train loss: 0.000924 |  Test loss: 0.001353 | l2: 6.776275\n",
      "epoch: 5400  | Train loss: 0.000996 |  Test loss: 0.001481 | l2: 6.776274\n",
      "epoch: 5600  | Train loss: 0.001048 |  Test loss: 0.001580 | l2: 6.776275\n",
      "epoch: 5800  | Train loss: 0.001133 |  Test loss: 0.001718 | l2: 6.776276\n",
      "epoch: 6000  | Train loss: 0.001196 |  Test loss: 0.001834 | l2: 6.776274\n",
      "epoch: 6200  | Train loss: 0.001229 |  Test loss: 0.001908 | l2: 6.776274\n",
      "epoch: 6400  | Train loss: 0.001327 |  Test loss: 0.002083 | l2: 6.776275\n",
      "epoch: 6600  | Train loss: 0.001340 |  Test loss: 0.002084 | l2: 6.776275\n",
      "epoch: 6800  | Train loss: 0.001303 |  Test loss: 0.002009 | l2: 6.776275\n",
      "epoch: 7000  | Train loss: 0.001254 |  Test loss: 0.001951 | l2: 6.776275\n",
      "epoch: 7200  | Train loss: 0.001272 |  Test loss: 0.002003 | l2: 6.776275\n",
      "epoch: 7400  | Train loss: 0.001328 |  Test loss: 0.002115 | l2: 6.776276\n",
      "epoch: 7600  | Train loss: 0.001399 |  Test loss: 0.002275 | l2: 6.776275\n",
      "epoch: 7800  | Train loss: 0.001497 |  Test loss: 0.002472 | l2: 6.776276\n",
      "epoch: 8000  | Train loss: 0.001506 |  Test loss: 0.002501 | l2: 6.776275\n",
      "epoch: 8200  | Train loss: 0.001438 |  Test loss: 0.002380 | l2: 6.776274\n",
      "epoch: 8400  | Train loss: 0.001391 |  Test loss: 0.002257 | l2: 6.776275\n",
      "epoch: 8600  | Train loss: 0.001398 |  Test loss: 0.002252 | l2: 6.776275\n",
      "epoch: 8800  | Train loss: 0.001417 |  Test loss: 0.002276 | l2: 6.776275\n",
      "epoch: 9000  | Train loss: 0.001431 |  Test loss: 0.002275 | l2: 6.776275\n",
      "epoch: 9200  | Train loss: 0.001469 |  Test loss: 0.002340 | l2: 6.776276\n",
      "epoch: 9400  | Train loss: 0.001493 |  Test loss: 0.002397 | l2: 6.776275\n",
      "epoch: 9600  | Train loss: 0.001440 |  Test loss: 0.002338 | l2: 6.776274\n",
      "epoch: 9800  | Train loss: 0.001398 |  Test loss: 0.002266 | l2: 6.776275\n",
      "---------alpha=0.3981072008609772---------\n",
      "epoch: 0  | Train loss: 0.020002 |  Test loss: 0.020271 | l2: 17.021235\n",
      "epoch: 200  | Train loss: 0.000558 |  Test loss: 0.000786 | l2: 17.021233\n",
      "epoch: 400  | Train loss: 0.000495 |  Test loss: 0.000738 | l2: 17.021236\n",
      "epoch: 600  | Train loss: 0.000414 |  Test loss: 0.000671 | l2: 17.021235\n",
      "epoch: 800  | Train loss: 0.000324 |  Test loss: 0.000592 | l2: 17.021236\n",
      "epoch: 1000  | Train loss: 0.000249 |  Test loss: 0.000527 | l2: 17.021233\n",
      "epoch: 1200  | Train loss: 0.000201 |  Test loss: 0.000462 | l2: 17.021233\n",
      "epoch: 1400  | Train loss: 0.000162 |  Test loss: 0.000398 | l2: 17.021235\n",
      "epoch: 1600  | Train loss: 0.000132 |  Test loss: 0.000352 | l2: 17.021236\n",
      "epoch: 1800  | Train loss: 0.000109 |  Test loss: 0.000331 | l2: 17.021233\n",
      "epoch: 2000  | Train loss: 0.000092 |  Test loss: 0.000324 | l2: 17.021236\n",
      "epoch: 2200  | Train loss: 0.000080 |  Test loss: 0.000319 | l2: 17.021233\n",
      "epoch: 2400  | Train loss: 0.000072 |  Test loss: 0.000307 | l2: 17.021236\n",
      "epoch: 2600  | Train loss: 0.000067 |  Test loss: 0.000288 | l2: 17.021235\n",
      "epoch: 2800  | Train loss: 0.000062 |  Test loss: 0.000266 | l2: 17.021233\n",
      "epoch: 3000  | Train loss: 0.000058 |  Test loss: 0.000249 | l2: 17.021233\n",
      "epoch: 3200  | Train loss: 0.000055 |  Test loss: 0.000238 | l2: 17.021236\n",
      "epoch: 3400  | Train loss: 0.000052 |  Test loss: 0.000231 | l2: 17.021233\n",
      "epoch: 3600  | Train loss: 0.000050 |  Test loss: 0.000230 | l2: 17.021236\n",
      "epoch: 3800  | Train loss: 0.000048 |  Test loss: 0.000232 | l2: 17.021235\n",
      "epoch: 4000  | Train loss: 0.000048 |  Test loss: 0.000235 | l2: 17.021235\n",
      "epoch: 4200  | Train loss: 0.000048 |  Test loss: 0.000237 | l2: 17.021233\n",
      "epoch: 4400  | Train loss: 0.000049 |  Test loss: 0.000237 | l2: 17.021235\n",
      "epoch: 4600  | Train loss: 0.000051 |  Test loss: 0.000238 | l2: 17.021235\n",
      "epoch: 4800  | Train loss: 0.000052 |  Test loss: 0.000239 | l2: 17.021233\n",
      "epoch: 5000  | Train loss: 0.000053 |  Test loss: 0.000249 | l2: 17.021235\n",
      "epoch: 5200  | Train loss: 0.000055 |  Test loss: 0.000258 | l2: 17.021235\n",
      "epoch: 5400  | Train loss: 0.000057 |  Test loss: 0.000260 | l2: 17.021235\n",
      "epoch: 5600  | Train loss: 0.000055 |  Test loss: 0.000259 | l2: 17.021236\n",
      "epoch: 5800  | Train loss: 0.000051 |  Test loss: 0.000249 | l2: 17.021235\n",
      "epoch: 6000  | Train loss: 0.000049 |  Test loss: 0.000240 | l2: 17.021233\n",
      "epoch: 6200  | Train loss: 0.000048 |  Test loss: 0.000240 | l2: 17.021236\n",
      "epoch: 6400  | Train loss: 0.000048 |  Test loss: 0.000244 | l2: 17.021235\n",
      "epoch: 6600  | Train loss: 0.000051 |  Test loss: 0.000251 | l2: 17.021236\n",
      "epoch: 6800  | Train loss: 0.000055 |  Test loss: 0.000259 | l2: 17.021235\n",
      "epoch: 7000  | Train loss: 0.000059 |  Test loss: 0.000265 | l2: 17.021236\n",
      "epoch: 7200  | Train loss: 0.000062 |  Test loss: 0.000263 | l2: 17.021235\n",
      "epoch: 7400  | Train loss: 0.000065 |  Test loss: 0.000254 | l2: 17.021236\n",
      "epoch: 7600  | Train loss: 0.000068 |  Test loss: 0.000258 | l2: 17.021233\n",
      "epoch: 7800  | Train loss: 0.000065 |  Test loss: 0.000271 | l2: 17.021231\n",
      "epoch: 8000  | Train loss: 0.000063 |  Test loss: 0.000278 | l2: 17.021236\n",
      "epoch: 8200  | Train loss: 0.000067 |  Test loss: 0.000283 | l2: 17.021235\n",
      "epoch: 8400  | Train loss: 0.000070 |  Test loss: 0.000292 | l2: 17.021236\n",
      "epoch: 8600  | Train loss: 0.000076 |  Test loss: 0.000294 | l2: 17.021233\n",
      "epoch: 8800  | Train loss: 0.000081 |  Test loss: 0.000310 | l2: 17.021236\n",
      "epoch: 9000  | Train loss: 0.000082 |  Test loss: 0.000316 | l2: 17.021235\n",
      "epoch: 9200  | Train loss: 0.000078 |  Test loss: 0.000301 | l2: 17.021236\n",
      "epoch: 9400  | Train loss: 0.000078 |  Test loss: 0.000305 | l2: 17.021233\n",
      "epoch: 9600  | Train loss: 0.000081 |  Test loss: 0.000310 | l2: 17.021233\n",
      "epoch: 9800  | Train loss: 0.000086 |  Test loss: 0.000333 | l2: 17.021235\n",
      "---------alpha=0.6309573650360107---------\n",
      "epoch: 0  | Train loss: 0.024721 |  Test loss: 0.023881 | l2: 42.755409\n",
      "epoch: 200  | Train loss: 0.000400 |  Test loss: 0.000639 | l2: 42.755413\n",
      "epoch: 400  | Train loss: 0.000273 |  Test loss: 0.000516 | l2: 42.755405\n",
      "epoch: 600  | Train loss: 0.000175 |  Test loss: 0.000403 | l2: 42.755402\n",
      "epoch: 800  | Train loss: 0.000115 |  Test loss: 0.000323 | l2: 42.755409\n",
      "epoch: 1000  | Train loss: 0.000080 |  Test loss: 0.000269 | l2: 42.755409\n",
      "epoch: 1200  | Train loss: 0.000056 |  Test loss: 0.000227 | l2: 42.755413\n",
      "epoch: 1400  | Train loss: 0.000040 |  Test loss: 0.000194 | l2: 42.755409\n",
      "epoch: 1600  | Train loss: 0.000030 |  Test loss: 0.000172 | l2: 42.755398\n",
      "epoch: 1800  | Train loss: 0.000024 |  Test loss: 0.000159 | l2: 42.755413\n",
      "epoch: 2000  | Train loss: 0.000020 |  Test loss: 0.000151 | l2: 42.755413\n",
      "epoch: 2200  | Train loss: 0.000017 |  Test loss: 0.000145 | l2: 42.755409\n",
      "epoch: 2400  | Train loss: 0.000014 |  Test loss: 0.000139 | l2: 42.755409\n",
      "epoch: 2600  | Train loss: 0.000013 |  Test loss: 0.000134 | l2: 42.755413\n",
      "epoch: 2800  | Train loss: 0.000011 |  Test loss: 0.000128 | l2: 42.755409\n",
      "epoch: 3000  | Train loss: 0.000010 |  Test loss: 0.000123 | l2: 42.755402\n",
      "epoch: 3200  | Train loss: 0.000008 |  Test loss: 0.000119 | l2: 42.755409\n",
      "epoch: 3400  | Train loss: 0.000007 |  Test loss: 0.000116 | l2: 42.755405\n",
      "epoch: 3600  | Train loss: 0.000006 |  Test loss: 0.000114 | l2: 42.755405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3800  | Train loss: 0.000005 |  Test loss: 0.000112 | l2: 42.755413\n",
      "epoch: 4000  | Train loss: 0.000005 |  Test loss: 0.000110 | l2: 42.755405\n",
      "epoch: 4200  | Train loss: 0.000004 |  Test loss: 0.000109 | l2: 42.755405\n",
      "epoch: 4400  | Train loss: 0.000004 |  Test loss: 0.000108 | l2: 42.755409\n",
      "epoch: 4600  | Train loss: 0.000003 |  Test loss: 0.000107 | l2: 42.755405\n",
      "epoch: 4800  | Train loss: 0.000003 |  Test loss: 0.000106 | l2: 42.755409\n",
      "epoch: 5000  | Train loss: 0.000003 |  Test loss: 0.000105 | l2: 42.755405\n",
      "epoch: 5200  | Train loss: 0.000002 |  Test loss: 0.000104 | l2: 42.755409\n",
      "epoch: 5400  | Train loss: 0.000002 |  Test loss: 0.000102 | l2: 42.755405\n",
      "epoch: 5600  | Train loss: 0.000002 |  Test loss: 0.000101 | l2: 42.755402\n",
      "epoch: 5800  | Train loss: 0.000002 |  Test loss: 0.000100 | l2: 42.755405\n",
      "epoch: 6000  | Train loss: 0.000002 |  Test loss: 0.000098 | l2: 42.755417\n",
      "epoch: 6200  | Train loss: 0.000002 |  Test loss: 0.000096 | l2: 42.755413\n",
      "epoch: 6400  | Train loss: 0.000002 |  Test loss: 0.000096 | l2: 42.755402\n",
      "epoch: 6600  | Train loss: 0.000001 |  Test loss: 0.000094 | l2: 42.755409\n",
      "epoch: 6800  | Train loss: 0.000001 |  Test loss: 0.000093 | l2: 42.755409\n",
      "epoch: 7000  | Train loss: 0.000001 |  Test loss: 0.000092 | l2: 42.755405\n",
      "epoch: 7200  | Train loss: 0.000001 |  Test loss: 0.000091 | l2: 42.755409\n",
      "epoch: 7400  | Train loss: 0.000001 |  Test loss: 0.000091 | l2: 42.755417\n",
      "epoch: 7600  | Train loss: 0.000001 |  Test loss: 0.000090 | l2: 42.755409\n",
      "epoch: 7800  | Train loss: 0.000001 |  Test loss: 0.000089 | l2: 42.755405\n",
      "epoch: 8000  | Train loss: 0.000001 |  Test loss: 0.000089 | l2: 42.755398\n",
      "epoch: 8200  | Train loss: 0.000001 |  Test loss: 0.000088 | l2: 42.755409\n",
      "epoch: 8400  | Train loss: 0.000001 |  Test loss: 0.000088 | l2: 42.755409\n",
      "epoch: 8600  | Train loss: 0.000001 |  Test loss: 0.000087 | l2: 42.755405\n",
      "epoch: 8800  | Train loss: 0.000001 |  Test loss: 0.000087 | l2: 42.755409\n",
      "epoch: 9000  | Train loss: 0.000001 |  Test loss: 0.000087 | l2: 42.755405\n",
      "epoch: 9200  | Train loss: 0.000001 |  Test loss: 0.000087 | l2: 42.755409\n",
      "epoch: 9400  | Train loss: 0.000001 |  Test loss: 0.000086 | l2: 42.755405\n",
      "epoch: 9600  | Train loss: 0.000001 |  Test loss: 0.000086 | l2: 42.755409\n",
      "epoch: 9800  | Train loss: 0.000001 |  Test loss: 0.000086 | l2: 42.755409\n",
      "---------alpha=1.0---------\n",
      "epoch: 0  | Train loss: 0.048158 |  Test loss: 0.043588 | l2: 107.396713\n",
      "epoch: 200  | Train loss: 0.000268 |  Test loss: 0.000627 | l2: 107.396698\n",
      "epoch: 400  | Train loss: 0.000110 |  Test loss: 0.000415 | l2: 107.396690\n",
      "epoch: 600  | Train loss: 0.000061 |  Test loss: 0.000328 | l2: 107.396713\n",
      "epoch: 800  | Train loss: 0.000039 |  Test loss: 0.000279 | l2: 107.396713\n",
      "epoch: 1000  | Train loss: 0.000028 |  Test loss: 0.000246 | l2: 107.396729\n",
      "epoch: 1200  | Train loss: 0.000021 |  Test loss: 0.000221 | l2: 107.396713\n",
      "epoch: 1400  | Train loss: 0.000016 |  Test loss: 0.000201 | l2: 107.396713\n",
      "epoch: 1600  | Train loss: 0.000013 |  Test loss: 0.000187 | l2: 107.396698\n",
      "epoch: 1800  | Train loss: 0.000010 |  Test loss: 0.000176 | l2: 107.396698\n",
      "epoch: 2000  | Train loss: 0.000008 |  Test loss: 0.000167 | l2: 107.396713\n",
      "epoch: 2200  | Train loss: 0.000007 |  Test loss: 0.000160 | l2: 107.396721\n",
      "epoch: 2400  | Train loss: 0.000006 |  Test loss: 0.000153 | l2: 107.396706\n",
      "epoch: 2600  | Train loss: 0.000005 |  Test loss: 0.000147 | l2: 107.396721\n",
      "epoch: 2800  | Train loss: 0.000004 |  Test loss: 0.000141 | l2: 107.396713\n",
      "epoch: 3000  | Train loss: 0.000004 |  Test loss: 0.000135 | l2: 107.396706\n",
      "epoch: 3200  | Train loss: 0.000003 |  Test loss: 0.000130 | l2: 107.396706\n",
      "epoch: 3400  | Train loss: 0.000003 |  Test loss: 0.000124 | l2: 107.396713\n",
      "epoch: 3600  | Train loss: 0.000002 |  Test loss: 0.000119 | l2: 107.396713\n",
      "epoch: 3800  | Train loss: 0.000002 |  Test loss: 0.000114 | l2: 107.396713\n",
      "epoch: 4000  | Train loss: 0.000002 |  Test loss: 0.000110 | l2: 107.396721\n",
      "epoch: 4200  | Train loss: 0.000002 |  Test loss: 0.000106 | l2: 107.396706\n",
      "epoch: 4400  | Train loss: 0.000001 |  Test loss: 0.000103 | l2: 107.396706\n",
      "epoch: 4600  | Train loss: 0.000001 |  Test loss: 0.000100 | l2: 107.396713\n",
      "epoch: 4800  | Train loss: 0.000001 |  Test loss: 0.000098 | l2: 107.396698\n",
      "epoch: 5000  | Train loss: 0.000001 |  Test loss: 0.000096 | l2: 107.396698\n",
      "epoch: 5200  | Train loss: 0.000001 |  Test loss: 0.000095 | l2: 107.396713\n",
      "epoch: 5400  | Train loss: 0.000001 |  Test loss: 0.000092 | l2: 107.396713\n",
      "epoch: 5600  | Train loss: 0.000001 |  Test loss: 0.000090 | l2: 107.396713\n",
      "epoch: 5800  | Train loss: 0.000001 |  Test loss: 0.000091 | l2: 107.396713\n",
      "epoch: 6000  | Train loss: 0.000001 |  Test loss: 0.000089 | l2: 107.396729\n",
      "epoch: 6200  | Train loss: 0.000001 |  Test loss: 0.000087 | l2: 107.396729\n",
      "epoch: 6400  | Train loss: 0.000001 |  Test loss: 0.000087 | l2: 107.396706\n",
      "epoch: 6600  | Train loss: 0.000000 |  Test loss: 0.000086 | l2: 107.396713\n",
      "epoch: 6800  | Train loss: 0.000001 |  Test loss: 0.000085 | l2: 107.396698\n",
      "epoch: 7000  | Train loss: 0.000001 |  Test loss: 0.000083 | l2: 107.396698\n",
      "epoch: 7200  | Train loss: 0.000001 |  Test loss: 0.000083 | l2: 107.396721\n",
      "epoch: 7400  | Train loss: 0.000001 |  Test loss: 0.000083 | l2: 107.396706\n",
      "epoch: 7600  | Train loss: 0.000000 |  Test loss: 0.000083 | l2: 107.396706\n",
      "epoch: 7800  | Train loss: 0.000001 |  Test loss: 0.000083 | l2: 107.396713\n",
      "epoch: 8000  | Train loss: 0.000001 |  Test loss: 0.000080 | l2: 107.396698\n",
      "epoch: 8200  | Train loss: 0.000001 |  Test loss: 0.000082 | l2: 107.396690\n",
      "epoch: 8400  | Train loss: 0.000000 |  Test loss: 0.000080 | l2: 107.396713\n",
      "epoch: 8600  | Train loss: 0.000000 |  Test loss: 0.000080 | l2: 107.396713\n",
      "epoch: 8800  | Train loss: 0.000001 |  Test loss: 0.000080 | l2: 107.396706\n",
      "epoch: 9000  | Train loss: 0.000000 |  Test loss: 0.000079 | l2: 107.396698\n",
      "epoch: 9200  | Train loss: 0.000000 |  Test loss: 0.000079 | l2: 107.396713\n",
      "epoch: 9400  | Train loss: 0.000001 |  Test loss: 0.000080 | l2: 107.396713\n",
      "epoch: 9600  | Train loss: 0.000001 |  Test loss: 0.000079 | l2: 107.396698\n",
      "epoch: 9800  | Train loss: 0.000000 |  Test loss: 0.000078 | l2: 107.396729\n",
      "---------alpha=1.5848931074142456---------\n",
      "epoch: 0  | Train loss: 0.173149 |  Test loss: 0.163033 | l2: 269.768341\n",
      "epoch: 200  | Train loss: 0.000945 |  Test loss: 0.004946 | l2: 269.768311\n",
      "epoch: 400  | Train loss: 0.000345 |  Test loss: 0.003725 | l2: 269.768341\n",
      "epoch: 600  | Train loss: 0.000185 |  Test loss: 0.003148 | l2: 269.768341\n",
      "epoch: 800  | Train loss: 0.000110 |  Test loss: 0.002789 | l2: 269.768280\n",
      "epoch: 1000  | Train loss: 0.000070 |  Test loss: 0.002549 | l2: 269.768311\n",
      "epoch: 1200  | Train loss: 0.000048 |  Test loss: 0.002375 | l2: 269.768341\n",
      "epoch: 1400  | Train loss: 0.000035 |  Test loss: 0.002239 | l2: 269.768311\n",
      "epoch: 1600  | Train loss: 0.000026 |  Test loss: 0.002127 | l2: 269.768311\n",
      "epoch: 1800  | Train loss: 0.000020 |  Test loss: 0.002033 | l2: 269.768341\n",
      "epoch: 2000  | Train loss: 0.000015 |  Test loss: 0.001953 | l2: 269.768311\n",
      "epoch: 2200  | Train loss: 0.000012 |  Test loss: 0.001886 | l2: 269.768311\n",
      "epoch: 2400  | Train loss: 0.000009 |  Test loss: 0.001828 | l2: 269.768280\n",
      "epoch: 2600  | Train loss: 0.000007 |  Test loss: 0.001778 | l2: 269.768341\n",
      "epoch: 2800  | Train loss: 0.000006 |  Test loss: 0.001735 | l2: 269.768311\n",
      "epoch: 3000  | Train loss: 0.000004 |  Test loss: 0.001698 | l2: 269.768311\n",
      "epoch: 3200  | Train loss: 0.000003 |  Test loss: 0.001665 | l2: 269.768341\n",
      "epoch: 3400  | Train loss: 0.000003 |  Test loss: 0.001637 | l2: 269.768311\n",
      "epoch: 3600  | Train loss: 0.000002 |  Test loss: 0.001611 | l2: 269.768311\n",
      "epoch: 3800  | Train loss: 0.000002 |  Test loss: 0.001589 | l2: 269.768311\n",
      "epoch: 4000  | Train loss: 0.000001 |  Test loss: 0.001570 | l2: 269.768311\n",
      "epoch: 4200  | Train loss: 0.000001 |  Test loss: 0.001553 | l2: 269.768372\n",
      "epoch: 4400  | Train loss: 0.000001 |  Test loss: 0.001538 | l2: 269.768341\n",
      "epoch: 4600  | Train loss: 0.000001 |  Test loss: 0.001526 | l2: 269.768372\n",
      "epoch: 4800  | Train loss: 0.000000 |  Test loss: 0.001516 | l2: 269.768341\n",
      "epoch: 5000  | Train loss: 0.000000 |  Test loss: 0.001506 | l2: 269.768372\n",
      "epoch: 5200  | Train loss: 0.000000 |  Test loss: 0.001499 | l2: 269.768311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5400  | Train loss: 0.000000 |  Test loss: 0.001493 | l2: 269.768341\n",
      "epoch: 5600  | Train loss: 0.000000 |  Test loss: 0.001487 | l2: 269.768341\n",
      "epoch: 5800  | Train loss: 0.000000 |  Test loss: 0.001486 | l2: 269.768311\n",
      "epoch: 6000  | Train loss: 0.000000 |  Test loss: 0.001482 | l2: 269.768341\n",
      "epoch: 6200  | Train loss: 0.000001 |  Test loss: 0.001482 | l2: 269.768311\n",
      "epoch: 6400  | Train loss: 0.000004 |  Test loss: 0.001485 | l2: 269.768311\n",
      "epoch: 6600  | Train loss: 0.000002 |  Test loss: 0.001475 | l2: 269.768341\n",
      "epoch: 6800  | Train loss: 0.000001 |  Test loss: 0.001471 | l2: 269.768341\n",
      "epoch: 7000  | Train loss: 0.000000 |  Test loss: 0.001472 | l2: 269.768311\n",
      "epoch: 7200  | Train loss: 0.000000 |  Test loss: 0.001474 | l2: 269.768311\n",
      "epoch: 7400  | Train loss: 0.000000 |  Test loss: 0.001473 | l2: 269.768280\n",
      "epoch: 7600  | Train loss: 0.000000 |  Test loss: 0.001473 | l2: 269.768341\n",
      "epoch: 7800  | Train loss: 0.000001 |  Test loss: 0.001471 | l2: 269.768311\n",
      "epoch: 8000  | Train loss: 0.000001 |  Test loss: 0.001472 | l2: 269.768341\n",
      "epoch: 8200  | Train loss: 0.000001 |  Test loss: 0.001468 | l2: 269.768311\n",
      "epoch: 8400  | Train loss: 0.000000 |  Test loss: 0.001468 | l2: 269.768311\n",
      "epoch: 8600  | Train loss: 0.000000 |  Test loss: 0.001474 | l2: 269.768311\n",
      "epoch: 8800  | Train loss: 0.000001 |  Test loss: 0.001471 | l2: 269.768341\n",
      "epoch: 9000  | Train loss: 0.000001 |  Test loss: 0.001472 | l2: 269.768311\n",
      "epoch: 9200  | Train loss: 0.000001 |  Test loss: 0.001474 | l2: 269.768280\n",
      "epoch: 9400  | Train loss: 0.000000 |  Test loss: 0.001469 | l2: 269.768311\n",
      "epoch: 9600  | Train loss: 0.000000 |  Test loss: 0.001469 | l2: 269.768311\n",
      "epoch: 9800  | Train loss: 0.000001 |  Test loss: 0.001471 | l2: 269.768341\n",
      "---------alpha=2.5118863582611084---------\n",
      "epoch: 0  | Train loss: 0.720958 |  Test loss: 0.725327 | l2: 677.627441\n",
      "epoch: 200  | Train loss: 0.007940 |  Test loss: 0.058166 | l2: 677.627380\n",
      "epoch: 400  | Train loss: 0.002546 |  Test loss: 0.049173 | l2: 677.627441\n",
      "epoch: 600  | Train loss: 0.001105 |  Test loss: 0.045586 | l2: 677.627502\n",
      "epoch: 800  | Train loss: 0.000536 |  Test loss: 0.043764 | l2: 677.627441\n",
      "epoch: 1000  | Train loss: 0.000277 |  Test loss: 0.042709 | l2: 677.627441\n",
      "epoch: 1200  | Train loss: 0.000151 |  Test loss: 0.042040 | l2: 677.627441\n",
      "epoch: 1400  | Train loss: 0.000085 |  Test loss: 0.041589 | l2: 677.627380\n",
      "epoch: 1600  | Train loss: 0.000049 |  Test loss: 0.041273 | l2: 677.627319\n",
      "epoch: 1800  | Train loss: 0.000028 |  Test loss: 0.041051 | l2: 677.627380\n",
      "epoch: 2000  | Train loss: 0.000016 |  Test loss: 0.040897 | l2: 677.627380\n",
      "epoch: 2200  | Train loss: 0.000009 |  Test loss: 0.040792 | l2: 677.627380\n",
      "epoch: 2400  | Train loss: 0.000005 |  Test loss: 0.040723 | l2: 677.627319\n",
      "epoch: 2600  | Train loss: 0.000003 |  Test loss: 0.040680 | l2: 677.627441\n",
      "epoch: 2800  | Train loss: 0.000002 |  Test loss: 0.040653 | l2: 677.627319\n",
      "epoch: 3000  | Train loss: 0.000001 |  Test loss: 0.040635 | l2: 677.627380\n",
      "epoch: 3200  | Train loss: 0.000001 |  Test loss: 0.040624 | l2: 677.627380\n",
      "epoch: 3400  | Train loss: 0.000000 |  Test loss: 0.040617 | l2: 677.627441\n",
      "epoch: 3600  | Train loss: 0.000000 |  Test loss: 0.040612 | l2: 677.627441\n",
      "epoch: 3800  | Train loss: 0.000000 |  Test loss: 0.040608 | l2: 677.627441\n",
      "epoch: 4000  | Train loss: 0.000000 |  Test loss: 0.040605 | l2: 677.627380\n",
      "epoch: 4200  | Train loss: 0.000000 |  Test loss: 0.040603 | l2: 677.627380\n",
      "epoch: 4400  | Train loss: 0.000000 |  Test loss: 0.040602 | l2: 677.627441\n",
      "epoch: 4600  | Train loss: 0.000000 |  Test loss: 0.040601 | l2: 677.627441\n",
      "epoch: 4800  | Train loss: 0.000000 |  Test loss: 0.040601 | l2: 677.627380\n",
      "epoch: 5000  | Train loss: 0.000000 |  Test loss: 0.040601 | l2: 677.627380\n",
      "epoch: 5200  | Train loss: 0.000000 |  Test loss: 0.040601 | l2: 677.627380\n",
      "epoch: 5400  | Train loss: 0.000000 |  Test loss: 0.040601 | l2: 677.627380\n",
      "epoch: 5600  | Train loss: 0.000000 |  Test loss: 0.040601 | l2: 677.627380\n",
      "epoch: 5800  | Train loss: 0.000000 |  Test loss: 0.040601 | l2: 677.627441\n",
      "epoch: 6000  | Train loss: 0.000000 |  Test loss: 0.040598 | l2: 677.627380\n",
      "epoch: 6200  | Train loss: 0.000000 |  Test loss: 0.040600 | l2: 677.627441\n",
      "epoch: 6400  | Train loss: 0.000000 |  Test loss: 0.040600 | l2: 677.627441\n",
      "epoch: 6600  | Train loss: 0.000000 |  Test loss: 0.040602 | l2: 677.627380\n",
      "epoch: 6800  | Train loss: 0.000000 |  Test loss: 0.040602 | l2: 677.627563\n",
      "epoch: 7000  | Train loss: 0.000000 |  Test loss: 0.040602 | l2: 677.627441\n",
      "epoch: 7200  | Train loss: 0.000000 |  Test loss: 0.040602 | l2: 677.627441\n",
      "epoch: 7400  | Train loss: 0.000000 |  Test loss: 0.040611 | l2: 677.627380\n",
      "epoch: 7600  | Train loss: 0.000000 |  Test loss: 0.040600 | l2: 677.627380\n",
      "epoch: 7800  | Train loss: 0.000000 |  Test loss: 0.040600 | l2: 677.627380\n",
      "epoch: 8000  | Train loss: 0.000000 |  Test loss: 0.040600 | l2: 677.627380\n",
      "epoch: 8200  | Train loss: 0.000000 |  Test loss: 0.040600 | l2: 677.627441\n",
      "epoch: 8400  | Train loss: 0.000000 |  Test loss: 0.040600 | l2: 677.627441\n",
      "epoch: 8600  | Train loss: 0.000000 |  Test loss: 0.040604 | l2: 677.627380\n",
      "epoch: 8800  | Train loss: 0.000000 |  Test loss: 0.040604 | l2: 677.627380\n",
      "epoch: 9000  | Train loss: 0.000000 |  Test loss: 0.040604 | l2: 677.627441\n",
      "epoch: 9200  | Train loss: 0.000000 |  Test loss: 0.040604 | l2: 677.627380\n",
      "epoch: 9400  | Train loss: 0.000000 |  Test loss: 0.040604 | l2: 677.627441\n",
      "epoch: 9600  | Train loss: 0.000000 |  Test loss: 0.040604 | l2: 677.627441\n",
      "epoch: 9800  | Train loss: 0.000000 |  Test loss: 0.040637 | l2: 677.627380\n",
      "---------alpha=3.981071949005127---------\n",
      "epoch: 0  | Train loss: 2.593961 |  Test loss: 2.671500 | l2: 1702.123535\n",
      "epoch: 200  | Train loss: 0.051728 |  Test loss: 0.493991 | l2: 1702.123291\n",
      "epoch: 400  | Train loss: 0.010366 |  Test loss: 0.445109 | l2: 1702.123291\n",
      "epoch: 600  | Train loss: 0.003134 |  Test loss: 0.433392 | l2: 1702.123413\n",
      "epoch: 800  | Train loss: 0.001110 |  Test loss: 0.430056 | l2: 1702.123535\n",
      "epoch: 1000  | Train loss: 0.000415 |  Test loss: 0.429313 | l2: 1702.123413\n",
      "epoch: 1200  | Train loss: 0.000156 |  Test loss: 0.429349 | l2: 1702.123291\n",
      "epoch: 1400  | Train loss: 0.000057 |  Test loss: 0.429515 | l2: 1702.123535\n",
      "epoch: 1600  | Train loss: 0.000020 |  Test loss: 0.429625 | l2: 1702.123535\n",
      "epoch: 1800  | Train loss: 0.000007 |  Test loss: 0.429663 | l2: 1702.123657\n",
      "epoch: 2000  | Train loss: 0.000002 |  Test loss: 0.429658 | l2: 1702.123413\n",
      "epoch: 2200  | Train loss: 0.000001 |  Test loss: 0.429636 | l2: 1702.123291\n",
      "epoch: 2400  | Train loss: 0.000000 |  Test loss: 0.429613 | l2: 1702.123291\n",
      "epoch: 2600  | Train loss: 0.000000 |  Test loss: 0.429596 | l2: 1702.123535\n",
      "epoch: 2800  | Train loss: 0.000000 |  Test loss: 0.429585 | l2: 1702.123413\n",
      "epoch: 3000  | Train loss: 0.000000 |  Test loss: 0.429580 | l2: 1702.123535\n",
      "epoch: 3200  | Train loss: 0.000000 |  Test loss: 0.429577 | l2: 1702.123413\n",
      "epoch: 3400  | Train loss: 0.000000 |  Test loss: 0.429576 | l2: 1702.123413\n",
      "epoch: 3600  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123291\n",
      "epoch: 3800  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123291\n",
      "epoch: 4000  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 4200  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123291\n",
      "epoch: 4400  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 4600  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 4800  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 5000  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 5200  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 5400  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 5600  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 5800  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 6000  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 6200  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 6400  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 6600  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6800  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 7000  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 7200  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 7400  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 7600  | Train loss: 0.000000 |  Test loss: 0.429575 | l2: 1702.123413\n",
      "epoch: 7800  | Train loss: 0.000000 |  Test loss: 0.429519 | l2: 1702.123413\n",
      "epoch: 8000  | Train loss: 0.000000 |  Test loss: 0.429518 | l2: 1702.123413\n",
      "epoch: 8200  | Train loss: 0.000000 |  Test loss: 0.429518 | l2: 1702.123413\n",
      "epoch: 8400  | Train loss: 0.000000 |  Test loss: 0.429518 | l2: 1702.123413\n",
      "epoch: 8600  | Train loss: 0.000000 |  Test loss: 0.429518 | l2: 1702.123413\n",
      "epoch: 8800  | Train loss: 0.000000 |  Test loss: 0.429518 | l2: 1702.123413\n",
      "epoch: 9000  | Train loss: 0.000000 |  Test loss: 0.429469 | l2: 1702.123291\n",
      "epoch: 9200  | Train loss: 0.000000 |  Test loss: 0.429469 | l2: 1702.123413\n",
      "epoch: 9400  | Train loss: 0.000000 |  Test loss: 0.429469 | l2: 1702.123535\n",
      "epoch: 9600  | Train loss: 0.000000 |  Test loss: 0.429469 | l2: 1702.123535\n",
      "epoch: 9800  | Train loss: 0.000000 |  Test loss: 0.429469 | l2: 1702.123535\n",
      "---------alpha=6.309573650360107---------\n",
      "epoch: 0  | Train loss: 8.126369 |  Test loss: 8.375451 | l2: 4275.540527\n",
      "epoch: 200  | Train loss: 0.305536 |  Test loss: 2.804249 | l2: 4275.539551\n",
      "epoch: 400  | Train loss: 0.048835 |  Test loss: 2.573327 | l2: 4275.540527\n",
      "epoch: 600  | Train loss: 0.010426 |  Test loss: 2.480532 | l2: 4275.540039\n",
      "epoch: 800  | Train loss: 0.002302 |  Test loss: 2.446951 | l2: 4275.540527\n",
      "epoch: 1000  | Train loss: 0.000501 |  Test loss: 2.437535 | l2: 4275.540527\n",
      "epoch: 1200  | Train loss: 0.000108 |  Test loss: 2.436429 | l2: 4275.541016\n",
      "epoch: 1400  | Train loss: 0.000023 |  Test loss: 2.437351 | l2: 4275.540527\n",
      "epoch: 1600  | Train loss: 0.000005 |  Test loss: 2.438322 | l2: 4275.540527\n",
      "epoch: 1800  | Train loss: 0.000001 |  Test loss: 2.438937 | l2: 4275.540039\n",
      "epoch: 2000  | Train loss: 0.000000 |  Test loss: 2.439258 | l2: 4275.540039\n",
      "epoch: 2200  | Train loss: 0.000000 |  Test loss: 2.439402 | l2: 4275.540527\n",
      "epoch: 2400  | Train loss: 0.000000 |  Test loss: 2.439458 | l2: 4275.540527\n",
      "epoch: 2600  | Train loss: 0.000000 |  Test loss: 2.439479 | l2: 4275.540039\n",
      "epoch: 2800  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540527\n",
      "epoch: 3000  | Train loss: 0.000000 |  Test loss: 2.439489 | l2: 4275.540039\n",
      "epoch: 3200  | Train loss: 0.000000 |  Test loss: 2.439488 | l2: 4275.540039\n",
      "epoch: 3400  | Train loss: 0.000000 |  Test loss: 2.439488 | l2: 4275.540039\n",
      "epoch: 3600  | Train loss: 0.000000 |  Test loss: 2.439488 | l2: 4275.540039\n",
      "epoch: 3800  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 4000  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 4200  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 4400  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 4600  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 4800  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 5000  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 5200  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 5400  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 5600  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 5800  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 6000  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 6200  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 6400  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 6600  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 6800  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 7000  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 7200  | Train loss: 0.000000 |  Test loss: 2.439486 | l2: 4275.540039\n",
      "epoch: 7400  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 7600  | Train loss: 0.000000 |  Test loss: 2.439486 | l2: 4275.540039\n",
      "epoch: 7800  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 8000  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 8200  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 8400  | Train loss: 0.000000 |  Test loss: 2.439486 | l2: 4275.540039\n",
      "epoch: 8600  | Train loss: 0.000000 |  Test loss: 2.439486 | l2: 4275.540039\n",
      "epoch: 8800  | Train loss: 0.000000 |  Test loss: 2.439486 | l2: 4275.540039\n",
      "epoch: 9000  | Train loss: 0.000000 |  Test loss: 2.439487 | l2: 4275.540039\n",
      "epoch: 9200  | Train loss: 0.000000 |  Test loss: 2.439292 | l2: 4275.540039\n",
      "epoch: 9400  | Train loss: 0.000000 |  Test loss: 2.439299 | l2: 4275.540039\n",
      "epoch: 9600  | Train loss: 0.000000 |  Test loss: 2.439299 | l2: 4275.540039\n",
      "epoch: 9800  | Train loss: 0.000000 |  Test loss: 2.439299 | l2: 4275.540039\n",
      "---------alpha=10.0---------\n",
      "epoch: 0  | Train loss: 23.164505 |  Test loss: 24.451550 | l2: 10739.671875\n",
      "epoch: 200  | Train loss: 2.035864 |  Test loss: 11.571893 | l2: 10739.670898\n",
      "epoch: 400  | Train loss: 0.463350 |  Test loss: 10.419644 | l2: 10739.670898\n",
      "epoch: 600  | Train loss: 0.138704 |  Test loss: 10.014745 | l2: 10739.671875\n",
      "epoch: 800  | Train loss: 0.042456 |  Test loss: 9.790462 | l2: 10739.671875\n",
      "epoch: 1000  | Train loss: 0.014188 |  Test loss: 9.702000 | l2: 10739.670898\n",
      "epoch: 1200  | Train loss: 0.005277 |  Test loss: 9.663080 | l2: 10739.669922\n",
      "epoch: 1400  | Train loss: 0.002096 |  Test loss: 9.644765 | l2: 10739.671875\n",
      "epoch: 1600  | Train loss: 0.000863 |  Test loss: 9.635651 | l2: 10739.670898\n",
      "epoch: 1800  | Train loss: 0.000358 |  Test loss: 9.630799 | l2: 10739.671875\n",
      "epoch: 2000  | Train loss: 0.000145 |  Test loss: 9.628047 | l2: 10739.671875\n",
      "epoch: 2200  | Train loss: 0.000055 |  Test loss: 9.626410 | l2: 10739.670898\n",
      "epoch: 2400  | Train loss: 0.000019 |  Test loss: 9.625394 | l2: 10739.671875\n",
      "epoch: 2600  | Train loss: 0.000006 |  Test loss: 9.624775 | l2: 10739.671875\n",
      "epoch: 2800  | Train loss: 0.000002 |  Test loss: 9.624429 | l2: 10739.670898\n",
      "epoch: 3000  | Train loss: 0.000000 |  Test loss: 9.624202 | l2: 10739.670898\n",
      "epoch: 3200  | Train loss: 0.000000 |  Test loss: 9.624115 | l2: 10739.671875\n",
      "epoch: 3400  | Train loss: 0.000000 |  Test loss: 9.624061 | l2: 10739.670898\n",
      "epoch: 3600  | Train loss: 0.000000 |  Test loss: 9.624041 | l2: 10739.671875\n",
      "epoch: 3800  | Train loss: 0.000000 |  Test loss: 9.624024 | l2: 10739.670898\n",
      "epoch: 4000  | Train loss: 0.000000 |  Test loss: 9.624022 | l2: 10739.671875\n",
      "epoch: 4200  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n",
      "epoch: 4400  | Train loss: 0.000000 |  Test loss: 9.624021 | l2: 10739.671875\n",
      "epoch: 4600  | Train loss: 0.000000 |  Test loss: 9.624021 | l2: 10739.671875\n",
      "epoch: 4800  | Train loss: 0.000000 |  Test loss: 9.624021 | l2: 10739.671875\n",
      "epoch: 5000  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n",
      "epoch: 5200  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n",
      "epoch: 5400  | Train loss: 0.000000 |  Test loss: 9.624019 | l2: 10739.671875\n",
      "epoch: 5600  | Train loss: 0.000000 |  Test loss: 9.624019 | l2: 10739.671875\n",
      "epoch: 5800  | Train loss: 0.000000 |  Test loss: 9.624019 | l2: 10739.671875\n",
      "epoch: 6000  | Train loss: 0.000000 |  Test loss: 9.624019 | l2: 10739.671875\n",
      "epoch: 6200  | Train loss: 0.000000 |  Test loss: 9.624018 | l2: 10739.671875\n",
      "epoch: 6400  | Train loss: 0.000000 |  Test loss: 9.624019 | l2: 10739.671875\n",
      "epoch: 6600  | Train loss: 0.000000 |  Test loss: 9.624019 | l2: 10739.671875\n",
      "epoch: 6800  | Train loss: 0.000000 |  Test loss: 9.624019 | l2: 10739.671875\n",
      "epoch: 7000  | Train loss: 0.000000 |  Test loss: 9.624019 | l2: 10739.671875\n",
      "epoch: 7200  | Train loss: 0.000000 |  Test loss: 9.624019 | l2: 10739.671875\n",
      "epoch: 7400  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n",
      "epoch: 7600  | Train loss: 0.000000 |  Test loss: 9.624019 | l2: 10739.671875\n",
      "epoch: 7800  | Train loss: 0.000000 |  Test loss: 9.624019 | l2: 10739.671875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8000  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n",
      "epoch: 8200  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n",
      "epoch: 8400  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n",
      "epoch: 8600  | Train loss: 0.000000 |  Test loss: 9.624019 | l2: 10739.671875\n",
      "epoch: 8800  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n",
      "epoch: 9000  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n",
      "epoch: 9200  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n",
      "epoch: 9400  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n",
      "epoch: 9600  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n",
      "epoch: 9800  | Train loss: 0.000000 |  Test loss: 9.624020 | l2: 10739.671875\n"
     ]
    }
   ],
   "source": [
    "alphas = 10**torch.linspace(-1,1,steps=11)\n",
    "loss_train_alpha = []\n",
    "loss_test_alpha = []\n",
    "Trs = []\n",
    "l2_weights = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(\"---------alpha={}---------\".format(alpha))\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    student = Net()\n",
    "\n",
    "    init(student, alpha=alpha)\n",
    "    _, scale = L2(student)\n",
    "\n",
    "    epochs = 10000\n",
    "    log = 200\n",
    "\n",
    "    optimizer = torch.optim.Adam(student.parameters(), lr=3e-4)\n",
    "\n",
    "    losses_train = []\n",
    "    losses_test = []\n",
    "    l2s = []\n",
    "\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs_train = student(inputs_train)\n",
    "        loss_train = torch.mean((outputs_train-labels_train)**2)\n",
    "        outputs_test = student(inputs_test)\n",
    "        loss_test = torch.mean((outputs_test-labels_test)**2)\n",
    "        params, l2 = L2(student)\n",
    "        init2(student, alpha=torch.sqrt(scale/l2))\n",
    "        params, l2 = L2(student)\n",
    "\n",
    "        loss_train.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % log == 0:\n",
    "            print(\"epoch: %d  | Train loss: %.6f |  Test loss: %.6f | l2: %.6f\"%(epoch, loss_train.detach().numpy(), loss_test.detach().numpy(), l2.detach().numpy()))\n",
    "\n",
    "        losses_train.append(loss_train.detach().numpy())\n",
    "        losses_test.append(loss_test.detach().numpy())\n",
    "        l2s.append(l2.detach().numpy())\n",
    "        \n",
    "    loss_train_alpha.append(loss_train.detach().numpy())\n",
    "    loss_test_alpha.append(loss_test.detach().numpy())\n",
    "    \n",
    "    h = torch.autograd.functional.jacobian(func, tuple([_.view(-1) for _ in student.parameters()]))\n",
    "    for i in range(6):\n",
    "        if i == 0:\n",
    "            h_ = h[0]\n",
    "        else:\n",
    "            h_ = torch.cat([h_, h[i]], dim=1)\n",
    "\n",
    "    svs = torch.svd(h_).S.detach().numpy()\n",
    "    trJtJ = np.sum(svs**2)\n",
    "    Trs.append(trJtJ)\n",
    "    l2_weights.append(l2.detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4568ba57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'No regularization')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(alphas, np.array(loss_train_alpha)+1e-5, marker=\"o\")\n",
    "plt.plot(alphas, loss_test_alpha, marker=\"o\")\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend([\"Training\", \"Validation\"], fontsize=15)\n",
    "plt.xlabel(r\"$||w||_2$\", fontsize=20)\n",
    "plt.ylabel(\"Loss\", fontsize=20)\n",
    "plt.title(\"No regularization\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547dc2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
